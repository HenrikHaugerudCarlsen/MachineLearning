{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Importing libraries\n",
    "\n",
    "from imageio import imread\n",
    "\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from random import random, seed\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso,Ridge\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### FrankeFunction, from lecture notes\n",
    "def FrankeFunction(x,y):\n",
    "    term1 = 0.75*np.exp(-(0.25*(9*x-2)**2) - 0.25*((9*y-2)**2))\n",
    "    term2 = 0.75*np.exp(-((9*x+1)**2)/49.0 - 0.1*(9*y+1))\n",
    "    term3 = 0.5*np.exp(-(9*x-7)**2/4.0 - 0.25*((9*y-3)**2))\n",
    "    term4 = -0.2*np.exp(-(9*x-4)**2 - (9*y-7)**2)\n",
    "\n",
    "    return term1 + term2 + term3 + term4 \n",
    "\n",
    "\n",
    "#### Defining the R2 function, from lecture notes\n",
    "def R2(y_data, y_model):\n",
    "    return 1 - np.sum((y_data - y_model) ** 2) / np.sum((y_data - np.mean(y_data)) ** 2)\n",
    "\n",
    "\n",
    "#### Defining the Mean square error, from lecture notes\n",
    "def MSE(y,ytilde):\n",
    "    n = len(y)\n",
    "    return 1/n * np.sum(np.abs(y-ytilde)**2)\n",
    "\n",
    "\n",
    "\n",
    "#### Creating the design matrix, from lecture notes\n",
    "def create_X(x, y, n ):\n",
    "    if len(x.shape) > 1:\n",
    "        x = np.ravel(x)\n",
    "        y = np.ravel(y)\n",
    "\n",
    "    N = len(x)\n",
    "    l = int((n+1)*(n+2)/2) # Number of elements in beta\n",
    "    X = np.ones((N,l))\n",
    "\n",
    "    for i in range(1,n+1):\n",
    "        q = int((i)*(i+1)/2)\n",
    "        for k in range(i+1):\n",
    "            X[:,q+k] = (x**(i-k))*(y**k)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "### making the OCS regression\n",
    "def OLSmethod(X,z):\n",
    "    return np.linalg.pinv(X.T @ X) @ X.T @ z\n",
    "\n",
    "\n",
    "\n",
    "###Ridgeregression\n",
    "def ridgeregg(X,y, lmb = 0.0001):\n",
    "    XtX = X.T @ X\n",
    "    p =np.shape(XtX)[0]\n",
    "\n",
    "    return np.linalg.pinv(XtX +  lmb * np.identity(p)) @ X.T @ y\n",
    "\n",
    "###Lassoregression using sklern\n",
    "def lassoregg(X,y,lmb = 0.0001):\n",
    "    RegLasso = Lasso(lmb,fit_intercept=False,max_iter=10000)\n",
    "    RegLasso.fit(X,y)\n",
    "    return RegLasso.coef_\n",
    "    #pred_y = RegLasso.predict(X)\n",
    "    \n",
    "    \n",
    "\n",
    "##### Plots a surface. \n",
    "def surfaceplot(x,y,z):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "\n",
    "    # Plot the surface.\n",
    "    surf = ax.plot_surface(x, y, z, cmap=cm.coolwarm,\n",
    "                           linewidth=0, antialiased=False)\n",
    "\n",
    "    # Customize the z axis.\n",
    "    ax.set_zlim(-0.10, 1.40)\n",
    "    ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "    ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "\n",
    "    # Add a color bar which maps values to colors.\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "##### Makes nice output\n",
    "def printQ(xdata,xmodel):\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(f\"MSE = |{MSE(xdata,xmodel)}|, R2 = |{r2_score(xdata,xmodel)}|\")\n",
    "    print(\"--------------------------------------------------------\\n\")\n",
    "    \n",
    "##### calculating the MSE, bias and variance using the bootstrap method and OLS \n",
    "def BootstrapOLS(X_train,X_test,z_train,z_test,numberOfStraps):\n",
    "    MSEdeglisttest = np.zeros(numberOfStraps)\n",
    "    MSEdeglisttrain = np.zeros(numberOfStraps)\n",
    "    bias = np.zeros(numberOfStraps)\n",
    "    variance = np.zeros(numberOfStraps)\n",
    "    for i in range(numberOfStraps):\n",
    "        bootX,bootz = resample(X_train,z_train.reshape(-1,1))\n",
    "        bootbetaOLS = OLSmethod(bootX,bootz)\n",
    "        # Making out model and adding it to a list\n",
    "        ztilde = X_test @ bootbetaOLS\n",
    "        zpred = X_train @ bootbetaOLS\n",
    "        MSEdeglisttest[i] =MSE(z_test,ztilde)\n",
    "        MSEdeglisttrain[i]= MSE(z_train,zpred)\n",
    "        bias[i],variance[i] = biassVariance(z_test,zpred)\n",
    "    return np.mean(MSEdeglisttest), np.mean(MSEdeglisttrain), np.mean(bias), np.mean(variance)\n",
    "\n",
    "##### calculating the MSE, bias and variance using the bootstrap method and Ridge regression\n",
    "def BootstrapRidge(X_train,X_test,z_train,z_test,lam,numberOfStraps):\n",
    "    MSEdeglisttest = np.zeros(numberOfStraps)\n",
    "    MSEdeglisttrain = np.zeros(numberOfStraps)\n",
    "    bias = np.zeros(numberOfStraps)\n",
    "    variance = np.zeros(numberOfStraps)\n",
    "    for i in range(numberOfStraps):\n",
    "        bootX,bootz = resample(X_train,z_train.reshape(-1,1))\n",
    "        bootbetaRidge = ridgeregg(bootX,bootz,lam)\n",
    "        # Making out model and adding it to a list\n",
    "        ztilde = X_test @ bootbetaRidge\n",
    "        zpred = X_train @ bootbetaRidge\n",
    "        MSEdeglisttest[i] =MSE(z_test,ztilde)\n",
    "        MSEdeglisttrain[i]= MSE(z_train,zpred)\n",
    "        bias[i],variance[i] = biassVariance(z_test,ztilde)    \n",
    "    return np.mean(MSEdeglisttest), np.mean(MSEdeglisttrain), np.mean(bias), np.mean(variance)\n",
    "\n",
    "#### This function is writen to debug the lasso regression \n",
    "#### It is used to compare Skleanrs function with the one we made above\n",
    "def BootstrapRidge2(X,z,lam,numberOfStraps):\n",
    "    scaler = StandardScaler()\n",
    "    MSEdeglisttest = np.zeros(numberOfStraps)\n",
    "    MSEdeglisttrain = np.zeros(numberOfStraps)\n",
    "    bias = np.zeros(numberOfStraps)\n",
    "    variance = np.zeros(numberOfStraps)\n",
    "    X_train, X_test, z_train, z_test = train_test_split(X, z, test_size=0.2)\n",
    "###### Fiting and removing intercept    \n",
    "    scaler.fit(X_train[:,1:])\n",
    "    X_train_scaled = scaler.transform(X_train[:,1:])\n",
    "    X_test_scaled = scaler.transform(X_test[:,1:])\n",
    "\n",
    "    scaler.fit(z_train)\n",
    "    z_train_scaled = scaler.transform(z_train)\n",
    "    z_test_scaled = scaler.transform(z_test)\n",
    "    for i in range(numberOfStraps):\n",
    "        bootX,bootz = resample(X_train_scaled,z_train_scaled)\n",
    "        RegRig = Ridge(alpha=lam,max_iter = 50000)\n",
    "        RegRig.fit(bootX,bootz)\n",
    "        #Creating model and adding to list\n",
    "        ztilde = RegRig.predict(X_test_scaled)\n",
    "        zpred = RegRig.predict(X_train_scaled)\n",
    "##### Calculating the error on the scaled predictions\n",
    "        MSEdeglisttest[i] =MSE(z_test_scaled,ztilde)\n",
    "        MSEdeglisttrain[i]= MSE(z_train_scaled,zpred)\n",
    "        bias[i],variance[i] = biassVariance(z_test_scaled,ztilde)\n",
    "    return np.mean(MSEdeglisttest), np.mean(MSEdeglisttrain), np.mean(bias), np.mean(variance)\n",
    "\n",
    "####### Bootstrao for Lasso. Here the intercept is kept.\n",
    "def BootstrapLasso(X_train,X_test,z_train,z_test,lam,numberOfStraps):\n",
    "    MSEdeglisttest = np.zeros(numberOfStraps)\n",
    "    MSEdeglisttrain = np.zeros(numberOfStraps)\n",
    "    bias = np.zeros(numberOfStraps)\n",
    "    variance = np.zeros(numberOfStraps)\n",
    "    for i in range(numberOfStraps):\n",
    "        bootX,bootz = resample(X_train,z_train.reshape(-1,1))\n",
    "        RegLasso = Lasso(lam,fit_intercept=False,max_iter=5000)\n",
    "        RegLasso.fit(bootX,bootz)\n",
    "        #Creating model and adding to list\n",
    "        ztilde = RegLasso.predict(X_train)\n",
    "        zpred = RegLasso.predict(X_test)\n",
    "        MSEdeglisttest[i] =MSE(z_test,zpred)\n",
    "        MSEdeglisttrain[i]= MSE(z_train,ztilse)\n",
    "        bias[i],variance[i] = biassVariance(z_test,ztilde)\n",
    "    return np.mean(MSEdeglisttest), np.mean(MSEdeglisttrain), np.mean(bias), np.mean(variance)\n",
    "\n",
    "##### Bootstrap for lasso, In this function we scale the data and remove the intercept.\n",
    "##### This function was writen because we thougth the above function did not work,\n",
    "##### Turned out that it did.\n",
    "def BootstrapLasso2(X,z,lam,numberOfStraps):\n",
    "    scaler = StandardScaler()\n",
    "    MSEdeglisttest = np.zeros(numberOfStraps)\n",
    "    MSEdeglisttrain = np.zeros(numberOfStraps)\n",
    "    bias = np.zeros(numberOfStraps)\n",
    "    variance = np.zeros(numberOfStraps)\n",
    "    X_train, X_test, z_train, z_test = train_test_split(X, z, test_size=0.2)\n",
    "###### Fiting and removing intercept    \n",
    "    scaler.fit(X_train[:,1:])\n",
    "    X_train_scaled = scaler.transform(X_train[:,1:])\n",
    "    X_test_scaled = scaler.transform(X_test[:,1:])\n",
    "\n",
    "    scaler.fit(z_train)\n",
    "    z_train_scaled = scaler.transform(z_train)\n",
    "    z_test_scaled = scaler.transform(z_test)\n",
    "    for i in range(numberOfStraps):\n",
    "        bootX,bootz = resample(X_train_scaled,z_train_scaled)\n",
    "        RegLasso = Lasso(alpha=lam,max_iter = 50000)\n",
    "        RegLasso.fit(bootX,bootz)\n",
    "        #Creating model and adding to list\n",
    "        ztilde = RegLasso.predict(X_test_scaled)\n",
    "        zpred = RegLasso.predict(X_train_scaled)\n",
    "##### Calculating the error on the scaled predictions\n",
    "        MSEdeglisttest[i] =MSE(z_test_scaled,ztilde)\n",
    "        MSEdeglisttrain[i]= MSE(z_train_scaled,zpred)\n",
    "        bias[i],variance[i] = biassVariance(z_test_scaled,ztilde)\n",
    "    return np.mean(MSEdeglisttest), np.mean(MSEdeglisttrain), np.mean(bias), np.mean(variance)\n",
    "\n",
    "#### This function loops over polynomial degrees and calculate the MSE for OLS using bootstrap resampling\n",
    "def plotMSEcomplexity(x,y,z,n):\n",
    "    MSElisttest = []\n",
    "    MSElisttrain = []\n",
    "    biasList =[]\n",
    "    varianceList =[] \n",
    "    n = n+1\n",
    "    complexity = np.arange(n)\n",
    "    print(f\"The polynomial range is [{complexity[0]},{complexity[-1]}] \")\n",
    "    \n",
    "    # Running over the degrees of polynomails\n",
    "    for degree in complexity:\n",
    "        # Creating the designmatrix and splitting into train and test\n",
    "        X = create_X(x,y,degree)\n",
    "        X_train, X_test, z_train, z_test = train_test_split(X, z.reshape(-1,1), test_size=0.2)\n",
    "        \n",
    "        numberOfStraps = 100\n",
    "        MSE_train, MSE_test,bias,variance = BootstrapOLS(X_train,X_test,z_train,z_test,numberOfStraps)\n",
    "        \n",
    "        \n",
    "        # Appending the mean to the MSE list when the loop has run for its specific degree\n",
    "        MSElisttest.append(MSE_train)\n",
    "        MSElisttrain.append(MSE_test)\n",
    "        biasList.append(bias)\n",
    "        varianceList.append(variance)\n",
    "        \n",
    "    plt.title('ERROR')\n",
    "    plt.plot(complexity,MSElisttest,\"r\", label = \"test\")\n",
    "    plt.plot(complexity,MSElisttrain,\"k\", label = \"train\")\n",
    "    plt.xlabel(\"Polynomial degree\")\n",
    "    plt.grid()\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.title(\"Figure of the MSE as a function of the complexity of the model\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.title('Bias variance')\n",
    "    plt.plot(complexity,biasList,\"r\", label = \"bias\",ls='',marker='o')\n",
    "    plt.plot(complexity,varianceList,\"k\", label = \"variance\",ls='',marker='o')\n",
    "    plt.xlabel(\"Polynomial degree\")\n",
    "    plt.grid()\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.title(\"Figure of the MSE as a function of the complexity of the model\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "#####finding the confidence intervall for beta\n",
    "def confidense(y,X):\n",
    "    var = 1    # To simplify we have asumed that the variance in the data is 1. \n",
    "    XtX = np.linalg.pinv(X.T@X)\n",
    "    conf = 2*var*np.sqrt(np.diag(XtX))\n",
    "    return conf\n",
    "\n",
    "#### Calvulates the bias and the variance\n",
    "def biassVariance(y,y_pred): \n",
    "    mean_pred = np.mean(y_pred)\n",
    "    bias = np.mean((y - mean_pred)**2)\n",
    "    variance = np.mean((y_pred - mean_pred)**2)\n",
    "    return bias, variance\n",
    "\n",
    "##### Our own k_fold for OLS, this code is inspired by code provided in the lecture notes\n",
    "def k_foldOLS(Data, k,X): \n",
    " #### Splitting the data using sklearn's functionality\n",
    "    k_split = KFold(n_splits = k)\n",
    " #   \"CV to calculate MSE\"\n",
    "    k_scores= np.zeros(k)\n",
    "    i = 0\n",
    "    for k_train_index, k_test_index in k_split.split(X):\n",
    "        k_xtrain = X[k_train_index]\n",
    "        k_ytrain = Data[k_train_index]\n",
    "        \n",
    "        k_xtest = X[k_test_index]\n",
    "        k_ytest = Data[k_test_index]\n",
    "        beta_k_OLS = OLSmethod(k_xtrain, k_ytrain)\n",
    "        model_predict = k_xtest @ beta_k_OLS\n",
    "\n",
    "        k_scores[i] = MSE(k_ytest, model_predict)\n",
    "\n",
    "        i += 1\n",
    "    MSE_kfold = np.mean(k_scores)\n",
    "    return MSE_kfold\n",
    "\n",
    "#### Same as above, but for ridge\n",
    "def k_foldRigd(Data, k,X): \n",
    "#    \"Splitting the data\"\n",
    "    k_split = KFold(n_splits = k)\n",
    "#    \"CV to calculate MSE\"\n",
    "    k_scores= np.zeros(k)\n",
    "    i = 0\n",
    "    for k_train_index, k_test_index in k_split.split(X):\n",
    "        #print(k_train_index)\n",
    "        k_xtrain = X[k_train_index]\n",
    "        k_ytrain = Data[k_train_index]\n",
    "        \n",
    "       # print(k_xtrain)\n",
    "        \n",
    "        k_xtest = X[k_test_index]\n",
    "        k_ytest = Data[k_test_index]\n",
    "\n",
    "        #k_Xtrain = poly.fit_transform(k_xtrain[:, np.newaxis])\n",
    "   #     \"Finding betaOLS for each k\"\n",
    "        beta_k_ridge = ridgeregg(k_xtrain, k_ytrain)\n",
    "\n",
    "        #k_Xtest = poly.fit_transform(k_xtest[:, np.newaxis])\n",
    "        model_predict = k_xtest @ beta_k_ridge\n",
    "\n",
    "        k_scores[i] = MSE(k_ytest, model_predict)\n",
    "\n",
    "        i += 1\n",
    "  #  print(k_scores)\n",
    "    MSE_kfold = np.mean(k_scores)\n",
    "   # print('MSE for k-fold OLS')\n",
    "   # print(MSE_kfold)\n",
    "    return MSE_kfold\n",
    "\n",
    "##### Samme as above, but for lasso\n",
    "def k_foldLasso(Data, k,X,lam=0.001):\n",
    "   # \"Splitting the data\"\n",
    "    k_split = KFold(n_splits = k)\n",
    "   # \"CV to calculate MSE\"\n",
    "    k_scores= np.zeros(k)\n",
    "    i = 0\n",
    "    for k_train_index, k_test_index in k_split.split(X):\n",
    "        #print(k_train_index)\n",
    "        k_xtrain = X[k_train_index]\n",
    "        k_ytrain = Data[k_train_index]\n",
    "        \n",
    "       # print(k_xtrain)\n",
    "        \n",
    "        k_xtest = X[k_test_index]\n",
    "        k_ytest = Data[k_test_index]\n",
    "\n",
    "        #k_Xtrain = poly.fit_transform(k_xtrain[:, np.newaxis])\n",
    "  #      \"Finding betaOLS for each k\"\n",
    "        beta_k_Lasso = lassoregg(k_xtrain, k_ytrain,lam)\n",
    "\n",
    "        #k_Xtest = poly.fit_transform(k_xtest[:, np.newaxis])\n",
    "        model_predict = k_xtest @ beta_k_Lasso\n",
    "\n",
    "        k_scores[i] = MSE(k_ytest, model_predict)\n",
    "\n",
    "        i += 1\n",
    "    MSE_kfold = np.mean(k_scores)\n",
    "    return MSE_kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Load the terrain\n",
    "data = imread(\"SRTM_data_Norway_1.tif\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Show the terrain\n",
    "#print(type(data))\n",
    "#data = np.array(terrain1)\n",
    "#print(data.shape)\n",
    "#print(data)\n",
    "plt.figure()\n",
    "plt.title(\"Terrain over Norway 1\")\n",
    "plt.imshow(data, cmap=\"gray\")\n",
    "plt.xlabel(\"’X’\")\n",
    "plt.colorbar()\n",
    "plt.ylabel(\"’Y’\")\n",
    "plt.show()\"\"\"\n",
    "\n",
    "#### Deleting some of the datapoints to save time\n",
    "for i in range(8):\n",
    "        #print(data.shape)\n",
    "        data = np.delete(data, slice(None, None, 3), axis=0)\n",
    "        data = np.delete(data, slice(None, None, 3), axis=1)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "n,m = data.shape\n",
    "\n",
    "\n",
    "\n",
    "#### Make grud.\n",
    "x = np.linspace(0, 1801, m)\n",
    "y = np.linspace(0, 3601, n)\n",
    "\n",
    "#### Set this to 20 and you get a result worth using\n",
    "nmax = 20\n",
    "\n",
    "npoly1 = np.arange(1,nmax+1) \n",
    "\n",
    "x,y = np.meshgrid(x,y)\n",
    "\n",
    "\n",
    "z = data\n",
    "R2test = []\n",
    "R2train = []\n",
    "\n",
    "##### For the ordinary not bootstraped\n",
    "MSEtest = []\n",
    "MSEtrain = []\n",
    "\n",
    "###### For the bootstrap\n",
    "Boot_MSE_test = []\n",
    "Boot_MSE_train = []\n",
    "\n",
    "##### Looping over different polynomial degrees to find the best one for OLS\n",
    "for npoly in npoly1:\n",
    "    #print(f\"---{npoly}---\")\n",
    "    X = create_X(x,y,npoly)\n",
    "\n",
    "\n",
    "    X_train, X_test, z_train, z_test = train_test_split(X,z.reshape(-1,1), test_size = 0.2)\n",
    "\n",
    "    #Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.fit_transform(X_test)\n",
    "    z_train_scaled = scaler.fit_transform(z_train)\n",
    "    z_test_scaled = scaler.fit_transform(z_test)\n",
    "    \n",
    "    MSEtest1, MSEtrain1, boot, variance = BootstrapOLS(X_train_scaled,X_test_scaled,z_train_scaled,z_test_scaled,30)\n",
    "    Boot_MSE_train.append(MSEtrain1)\n",
    "    Boot_MSE_test.append(MSEtest1)\n",
    "    \n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    z_scaled = scaler.fit_transform(z)\n",
    "\n",
    "\n",
    "    beta = OLSmethod(X_train_scaled,z_train_scaled)\n",
    "\n",
    "#### Making the fit \n",
    "    ztilde = X_train_scaled @ beta\n",
    "\n",
    "    \n",
    "    zpred = X_test_scaled @ beta\n",
    "\n",
    "    \n",
    "    MSEtest.append(MSE(z_test_scaled,zpred))\n",
    "    MSEtrain.append(MSE(z_train_scaled,ztilde))\n",
    "    \n",
    "    R2test.append(R2(z_test_scaled,zpred))\n",
    "    R2train.append(R2(z_train_scaled,ztilde))\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#Ordinart\n",
    "plt.plot(npoly1,R2test, label =\" R2\")\n",
    "plt.plot(npoly1,MSEtest,label = \"MSE\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel(\"Polynomial degree\")\n",
    "plt.title(\"MSE and R2 for the test data as a function of polynomial degree\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(npoly1,R2train, label =\" R2\")\n",
    "plt.plot(npoly1,MSEtrain,label = \"MSE\")\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel(\"Polynomial degree\")\n",
    "plt.title(\"MSE and R2 for the training data as a function of polynomial degree\")\n",
    "plt.ylabel(\"Value\")\n",
    "\"\"\"\n",
    "#### Shows the test and train MSE vs degree\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"Polynomail degree\")\n",
    "plt.plot(npoly1,Boot_MSE_train,label = \"Train\")\n",
    "plt.plot(npoly1,Boot_MSE_test,label = \"Test\")\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"Task6OLSTrainTest.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### output the best degree\n",
    "bestdeg = np.argmin(Boot_MSE_test)\n",
    "print(bestdeg)\n",
    "\n",
    "\n",
    "\n",
    "###### Here we make a plot of the best data and the fit given by the best degree\n",
    "print(x.shape,y.shape)\n",
    "X = create_X(x,y,bestdeg)\n",
    "\n",
    "        \n",
    "X_scaled = scaler.fit_transform(X)\n",
    "z_scaled = scaler.fit_transform(z)\n",
    "print(X_scaled.shape,z_scaled.shape)\n",
    "beta = OLSmethod(X_scaled,z_scaled.reshape(-1,1))\n",
    "\n",
    "plt.imshow(z_scaled)\n",
    "c= plt.colorbar()\n",
    "c.set_label(r'$z$')\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Real_mountans.pdf')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(X_scaled.shape)\n",
    "plt.imshow((X_scaled@beta).reshape(z_scaled.shape))\n",
    "c= plt.colorbar()\n",
    "c.set_label(r'$z$')\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Our_fit_of_mountans.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Here we loop over different hyperparameters \\lambda and polynomial degree to find the best fit for Ridge and Lasso \n",
    "lam = np.logspace(-2,3,6)\n",
    "#n = 20\n",
    "#bdeg = 16\n",
    "#for Ridge\n",
    "\n",
    "\n",
    "#for Lasso\n",
    "\n",
    "TestMSElist = np.zeros((len(lam),len(npoly1)))\n",
    "TrainMSElist = np.zeros_like(TestMSElist)\n",
    "biaslist = np.zeros_like(TestMSElist)\n",
    "variancelist = np.zeros_like(TestMSElist)\n",
    "\n",
    "\n",
    "TestMSElist_L = np.zeros_like(TestMSElist)\n",
    "TrainMSElist_L = np.zeros_like(TestMSElist)\n",
    "biaslist_L = np.zeros_like(TestMSElist)\n",
    "variancelist_L = np.zeros_like(TestMSElist)\n",
    "\n",
    "for j,bdeg in np.ndenumerate(npoly1):\n",
    "    print(\"degree: \", j)\n",
    "    for i,alpha in np.ndenumerate(lam):\n",
    "        X = create_X(x,y,bdeg)\n",
    "        X_train, X_test, z_train, z_test = train_test_split(X,z.reshape(-1,1), test_size = 0.2)\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.fit_transform(X_test)\n",
    "        z_train_scaled = scaler.fit_transform(z_train)\n",
    "        z_test_scaled = scaler.fit_transform(z_test)\n",
    "\n",
    "\n",
    "        TestMSE, TrainMSE, bias, variance= BootstrapRidge(X_train_scaled,X_test_scaled,z_train_scaled,z_test_scaled,alpha,30)\n",
    "\n",
    "######## To avoid problems with sklearn the bootstrap function does the scaling inside #######\n",
    "        TestMSE_L, Train_MSE_L,bias_L,variance_L = BootstrapLasso2(X,z.reshape(-1,1),alpha,30)\n",
    "        \n",
    " \n",
    "        TestMSElist[i,j] = TestMSE\n",
    "        TrainMSElist[i,j] = TrainMSE\n",
    "        biaslist[i,j] = bias\n",
    "        variancelist[i,j] = variance\n",
    "        \n",
    "        TestMSElist_L[i,j] = TestMSE_L\n",
    "        TrainMSElist_L[i,j] = Train_MSE_L\n",
    "        biaslist_L[i,j] = bias_L\n",
    "        variancelist_L[i,j] = variance_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Finding the best degree :) \n",
    "ridgemin = np.unravel_index(TestMSElist.argmin(), TestMSElist.shape)[1] +1\n",
    "lassomin = np.unravel_index(TestMSElist_L.argmin(), TestMSElist_L.shape)[1] +1\n",
    "print(lassomin)\n",
    "print(ridgemin)\n",
    "\n",
    "##### making plots of MSE and biass/variance as a function of lambda for the best polynomial degree\n",
    "plt.semilogx(lam,variancelist_L[:,lassomin-1],\"ok\",label='var')\n",
    "plt.semilogx(lam,biaslist_L[:,lassomin-1],\"or\",label = \"bias\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"$\\lambda$\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"Task6Bias6LassoPoly{lassomin}_New.pdf\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.semilogx(lam,TestMSElist_L[:,lassomin-1], \"-o\" ,label = \"Test\")\n",
    "plt.semilogx(lam,TrainMSElist_L[:,lassomin-1], \"-o\",label = \"Train\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"$\\lambda$\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(f\"Task6MSELassoPoly{lassomin}_New.pdf\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plt.semilogx(lam,variancelist[:,ridgemin-1],\"ok\",label='var')\n",
    "plt.semilogx(lam,biaslist[:,ridgemin-1],\"or\",label = \"bias\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"$\\lambda$\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"Task6Bias6RidgePoly{ridgemin}_New.pdf\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.semilogx(lam,TestMSElist[:,ridgemin-1], \"-o\" ,label = \"Test\")\n",
    "plt.semilogx(lam,TrainMSElist[:,ridgemin-1], \"-o\",label = \"Train\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"$\\lambda$\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(f\"Task6MSERidgePoly{ridgemin}_New.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
