{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56693e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "45650ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.022500000000000006"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return np.exp(x)/(1+np.exp(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def neuralnet(x,f,network_shape):\n",
    "    #Network_shape is the shape of the number of nodes per layer so fex [2,1,6,4,osv]\n",
    "    # Moreover network shape is also everything AFTER the input layer\n",
    "    #Input\n",
    "    nlayers = len(network_shape)\n",
    "    \n",
    "    # Setting up the weights and the biases for each layer\n",
    "    weights = []\n",
    "    b = []\n",
    "    xdim = x.shape[1] # dimention\n",
    "    weights.append(np.random.randn(xdim,network_shape[0])) # initialize\n",
    "    # filling up with the general layers and weights and biases between them\n",
    "    for i in range(1,nlayers):\n",
    "        weights.append(np.random.randn(network_shape[i-1],network_shape[i]))\n",
    "    \n",
    "    for i in range(nlayers):\n",
    "        b.append(np.random.randn(1,network_shape[i]))\n",
    "    \n",
    "    return weights, b\n",
    "\n",
    "def feed_forward(x, weights, b):\n",
    "    Z = []\n",
    "    A = []\n",
    "    nlayers = len(b)\n",
    "    a_h = np.copy(x) # here we gather the data for the different layers as vectors\n",
    "    \n",
    "    #actually loop over the layers\n",
    "    for i in range(nlayers-1):\n",
    "        iweights = weights[i] #setting the weights for the layer\n",
    "        \n",
    "\n",
    "        \n",
    "        z_h = np.matmul(a_h, iweights) + b[i]\n",
    "        Z.append(z_h)\n",
    "        \n",
    "        a_h = f(z_h) #this ia a^l in mortens notes\n",
    "        A.append(a_h)\n",
    "    output = np.matmul(z_hp, weights[-1]) + b[-1] #this is z^L in mortens notes\n",
    "    return Z, A\n",
    "\n",
    "def CostFunc(y_tilde, y, alpha,W):\n",
    "    #Ensure np array\n",
    "    s = 0\n",
    "    for w in W:\n",
    "        w = np.array(w)\n",
    "        s += (w**2).flatten().sum()\n",
    "    \n",
    "        \n",
    "    return 0.5/(len(y_tilde))*np.sum((y_tilde-y)**2) + alpha*s\n",
    "\n",
    "\n",
    "def back_prop(x,y,weights, b):\n",
    "    Zl,Al = feed_forward(x,weights, b) #this is l not 1\n",
    "    \n",
    "    # error in the output layer\n",
    "    error_output = Zl[-1] - Y\n",
    "    # error in the hidden layer\n",
    "    error_hidden = np.matmul(error_output, output_weights.T) * a_h * (1 - a_h)\n",
    "    \n",
    "    # gradients for the output layer\n",
    "    output_weights_gradient = np.matmul(a_h.T, error_output)\n",
    "    output_bias_gradient = np.sum(error_output, axis=0)\n",
    "    \n",
    "    # gradient for the hidden layer\n",
    "    hidden_weights_gradient = np.matmul(X.T, error_hidden)\n",
    "    hidden_bias_gradient = np.sum(error_hidden, axis=0)\n",
    "\n",
    "    return output_weights_gradient, output_bias_gradient, hidden_weights_gradient, hidden_bias_gradient\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "netshape = [2,3,5,7,5,2]\n",
    "w,b = neuralnet(np.zeros((1,8)),sigmoid,netshape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f095d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
